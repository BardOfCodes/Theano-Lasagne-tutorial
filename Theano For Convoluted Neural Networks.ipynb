{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano for Convoluted Neural Networks\n",
    "\n",
    "Convolutional Neural Networks (CNN) are biologically-inspired variants of MLPs. From Hubel and Wiesel’s early work on the cat’s visual cortex [Hubel68], we know the visual cortex contains a complex arrangement of cells. These cells are sensitive to small sub-regions of the visual field, called a receptive field. The sub-regions are tiled to cover the entire visual field. These cells act as local filters over the input space and are well-suited to exploit the strong spatially local correlation present in natural images.\n",
    "\n",
    "Additionally, two basic cell types have been identified: Simple cells respond maximally to specific edge-like patterns within their receptive field. Complex cells have larger receptive fields and are locally invariant to the exact position of the pattern.\n",
    "\n",
    "## Sparse Connectivity\n",
    "CNNs exploit spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. In other words, the inputs of hidden units in layer $m$ are from a subset of units in layer $m-1$, units that have spatially contiguous receptive fields. We can illustrate this graphically as follows:\n",
    "\n",
    "![Alt text](images/sparse_1D_nn.png)\n",
    "\n",
    "Imagine that layer $m-1$ is the input retina. In the above figure, units in layer $m$ have receptive fields of width 3 in the input retina and are thus only connected to 3 adjacent neurons in the retina layer. Units in layer $m+1$ have a similar connectivity with the layer below. We say that their receptive field with respect to the layer below is also 3, but their receptive field with respect to the input is larger (5). Each unit is unresponsive to variations outside of its receptive field with respect to the retina. The architecture thus ensures that the learnt “filters” produce the strongest response to a spatially local input pattern.\n",
    "\n",
    "However, as shown above, stacking many such layers leads to (non-linear) “filters” that become increasingly “global” (i.e. responsive to a larger region of pixel space). For example, the unit in hidden layer $m+1$ can encode a non-linear feature of width 5 (in terms of pixel space).\n",
    "\n",
    "## Shared Weights\n",
    "In addition, in CNNs, each filter $h_i$ is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map.\n",
    "\n",
    "## Details and Notations\n",
    "\n",
    "A feature map is obtained by repeated application of a function across sub-regions of the entire image, in other words, by convolution of the input image with a linear filter, adding a bias term and then applying a non-linear function. If we denote the k-th feature map at a given layer as $h^k$, whose filters are determined by the weights $W^k$ and bias $b_k$, then the feature map $h^k$ is obtained by\n",
    "\n",
    "\\begin{equation}\n",
    "h^k_{ij} = non-linear-function ( (W^k * x)_{ij} + b_k ).\n",
    "\\end{equation}\n",
    "\n",
    "To form a richer representation of the data, each hidden layer is composed of multiple feature maps, \\{h^{(k)}, k=0..K\\}. The weights W of a hidden layer can be represented in a 4D tensor containing elements for every combination of destination feature map, source feature map, source vertical position, and source horizontal position. The biases b can be represented as a vector containing one element for every destination feature map.\n",
    "\n",
    "## The Convolution Operator\n",
    "\n",
    "ConvOp is the main workhorse for implementing a convolutional layer in Theano. ConvOp is used by <code>theano.tensor.signal.conv2d</code>, which takes two symbolic inputs:\n",
    "\n",
    "1) a 4D tensor corresponding to a mini-batch of input images. The shape of the tensor is as follows: [mini-batch size, number of input feature maps, image height, image width].\n",
    "\n",
    "2) a 4D tensor corresponding to the weight matrix $W$. The shape of the tensor is: [number of feature maps at layer m, number of feature maps at layer $m-1$, filter height, filter width]\n",
    "\n",
    "### Now lets get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "D:\\Anaconda2\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "# Loading required directories\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There is only a slight difference in the code of MLP and CNN, hence we will only highlight the parts that are different\n",
    "\n",
    "network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28))\n",
    "# This time we do not apply input dropout, as it tends to work less well\n",
    "# for convolutional layers.\n",
    "\n",
    "# Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
    "# convolutions are supported as well; see the docstring.\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "        network, num_filters=32, filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "# Expert note: Lasagne provides alternative convolutional layers that\n",
    "# override Theano's choice of which implementation to use; for details\n",
    "# please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "# Max-pooling layer of factor 2 in both dimensions:\n",
    "network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "# Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "        network, num_filters=32, filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "# A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "network = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(network, p=.5),\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "# And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "network = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(network, p=.5),\n",
    "        num_units=10,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahead everything is similar to the MLP notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
