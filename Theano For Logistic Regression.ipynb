{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST digits using Logistic Regression\n",
    "\n",
    "This notebook will show how Theano can be used to implement the logistic regression. As the plan, we have,\n",
    "\n",
    "1) A brief Intro to logistic Regression\n",
    "\n",
    "2) Loading data models\n",
    "\n",
    "3) Making the Logistic Regression model\n",
    "\n",
    "4) Running the model\n",
    "\n",
    "Almost all of the material here can be found [here](http://deeplearning.net/tutorial/). This work is entirly baed on this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "So, What is Logistic regression?\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "Mathematically, the probability that an input vector $x$ is a member of a class $i$, a value of a stochastic variable $Y$, can be written as:\n",
    "\\begin{align}\n",
    "P(Y = i \\mid x, W,b) &= softmax_i (Wx+b)\\\\\n",
    "&= \\frac{e^{W_i x + b_i}}{\\sum_{j}{{e}^{W_j x + b_j }}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "\\begin{equation}\n",
    "y_{pred} = argmax_i P(Y = i \\mid x,W,b)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We thus maximize the log-likelihood of our classifier given all the labels in a training set.\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) =\n",
    "    \\sum_{i=0}^{|\\mathcal{D}|} \\log P(Y=y^{(i)} | x^{(i)}, \\theta)\n",
    "\\end{equation}\n",
    "The likelihood of the correct class is not the same as the number of right predictions, but from the point of view of a randomly initialized classifier they are pretty similar. \n",
    "\n",
    "Since we usually speak in terms of minimizing a loss function, learning will thus attempt to minimize the negative log-likelihood (NLL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "What is ordinary gradient descent? it is a simple algorithm in which we repeatedly make small steps downward on an error surface defined by a loss function of some parameters. For the purpose of ordinary gradient descent we consider that the training data is rolled into the loss function.\n",
    "\n",
    "Stochastic gradient descent (SGD) works according to the same principles as ordinary gradient descent, but proceeds more quickly by estimating the gradient from just a few examples at a time instead of the entire training set. In its purest form, we estimate the gradient from just a single example at a time.\n",
    "\n",
    "The variant that is generally recommended for deep learning is a further twist on stochastic gradient descent using so-called “minibatches”. Minibatch SGD (MSGD) works identically to SGD, except that we use more than one training example to make each estimate of the gradient. This technique reduces variance in the estimate of the gradient, and often makes better use of the hierarchical memory organization in modern computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "while training our model from data, we are trying to prepare it to do well on new examples, not the ones it has already seen. The training loop above for MSGD does not take this into account, and may overfit the training examples. A way to combat overfitting is through regularization.\n",
    "\n",
    "#### L2 regularization\n",
    "L2 regularization involve adding an extra term to the loss function, which penalizes certain parameter configurations.\n",
    "\n",
    "If our loss function is\n",
    "\\begin{equation}\n",
    "NLL(\\theta, \\mathcal{D}) = - \\sum_{i=0}^{|\\mathcal{D}|} \\log P(Y=y^{(i)} | x^{(i)}, \\theta)\n",
    "\\end{equation}\n",
    "then the regularized loss will be:\n",
    "\\begin{equation}\n",
    "E(\\theta, \\mathcal{D}) =  NLL(\\theta, \\mathcal{D}) + \\lambda||\\theta||_2^2\n",
    "\\end{equation}\n",
    "where, $||\\theta||$ is the $L_2$ norm of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "You can either download the data from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data), or from [The MNIST Database website](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "However, to understand the use of pickled data, we will load it from the existing picked file, [mnist.pkl.gz](mnist.pkl.gz)\n",
    "\n",
    "## Pickle \n",
    "\n",
    "Pickle is used for serializing and de-serializing a Python object structure. Any object in python can be pickled so that it can be saved on disk. For more information, check [this](https://pythontips.com/2013/08/02/what-is-pickle-in-python/) out.\n",
    "\n",
    "#### Now lets get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the useful libraries and packages\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import csv\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first define the Logistic regression class.\n",
    "It will have some basic definations,\n",
    "\n",
    "1) <code>\\_\\_init\\_\\_</code> : for the initialization. According the the number of inputs and outputs.\n",
    "\n",
    "2) <code> negative_log_likelihood </code> : For calculating the mean loss with respect to a given set of points of outcome variables(Minibatch).\n",
    "\n",
    "3)<code> error </code> : This gives the proportion of incorrectly labelled points.\n",
    "\n",
    "Kindly check the file, [LogisticRegression.py](code/LogisticRegression.py) to see how it has been coded. We will simply import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we will start the main processing.\n",
    "\n",
    "The steps involved are\n",
    "\n",
    "1) first\n",
    "\n",
    "2) second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we set some hyper parameters.\n",
    "learning_rate = 0.13\n",
    "n_epochs = 1000\n",
    "datasets = \"mnist.pkl.gz\"\n",
    "batch_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = csv.reader(open(\"train.csv\"))\n",
    "header = csv_file.next()\n",
    "data = []\n",
    "for row in csv_file:\n",
    "    data.append(row)\n",
    "data_full = numpy.array(data, dtype = \"float32\")\n",
    "datax = data_full[:,1:]\n",
    "datay = data_full[:,0:1]\n",
    "train_x = datax[0:37000,:]\n",
    "train_y = datay[0:37000,:]\n",
    "train_y.shape =(int(numpy.shape(train_y)[0]))\n",
    "validate_x =datax[37000:,:]\n",
    "validate_y = datay[37000:,:]\n",
    "validate_y.shape =(5000)\n",
    "train_set_x = theano.shared(numpy.asarray(train_x,\n",
    "                        dtype=theano.config.floatX),\n",
    "                        borrow=True)\n",
    "train_set_y = theano.shared(numpy.asarray(train_y,\n",
    "                        dtype=theano.config.floatX),\n",
    "                        borrow=True)\n",
    "train_set_y = T.cast(train_set_y,'int32')\n",
    "valid_set_x = theano.shared(numpy.asarray(validate_x,\n",
    "                        dtype=theano.config.floatX),\n",
    "                        borrow=True)\n",
    "valid_set_y = theano.shared(numpy.asarray(validate_y,\n",
    "                        dtype=theano.config.floatX),\n",
    "                        borrow=True)\n",
    "valid_set_y = T.cast(valid_set_y,'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "print(n_train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(n_valid_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = T.lscalar()\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "classifier = LR.LogisticRegression(input=x, n_in = 28*28 , n_out = 10)\n",
    "# You can check different features of the class.\n",
    "#classifier.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We set the total Loss/ the cost to be the negative Log-likelihood of the function\n",
    "cost = classifier.negative_log_likelihood(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.tensor.var.TensorVariable"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(valid_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we make a function to find the error we make in the mini batches we take\n",
    "#test_model = theano.function(\n",
    "#        inputs=[index],\n",
    "#        outputs=classifier.errors(y),\n",
    "#        givens={\n",
    "#            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#        }\n",
    "#    )\n",
    "# similarly\n",
    "validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the gradient of cost with respect to theta = (W,b)\n",
    "g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "# For looking at the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, for the update step\n",
    "# where classifier.W and classifier.b are shared theano tensors.\n",
    "\n",
    "updates = [(classifier.W, classifier.W - learning_rate * g_W),(classifier.b, classifier.b - learning_rate * g_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the coolest part.\n",
    "# compiling a Theano function `train_model` that returns the cost, but in\n",
    "# the same time updates the parameter of the model based on the rules\n",
    "# defined in `updates`\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")\n",
    "# We have defined all the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define the stopping criteria based on patientce.\n",
    "# early-stopping parameters\n",
    "patience = 5000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                       # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                              # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                              # go through this many\n",
    "                              # minibatche before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the parameters\n",
    "best_validation_loss = numpy.inf\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "done_looping = False\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63, minibatch 61/61, validation error 9.750000 %\n",
      "epoch 64, minibatch 61/61, validation error 10.208333 %\n",
      "epoch 65, minibatch 61/61, validation error 9.208333 %\n",
      "epoch 66, minibatch 61/61, validation error 9.791667 %\n",
      "epoch 67, minibatch 61/61, validation error 9.916667 %\n",
      "epoch 68, minibatch 61/61, validation error 12.541667 %\n",
      "epoch 69, minibatch 61/61, validation error 16.187500 %\n",
      "epoch 70, minibatch 61/61, validation error 9.187500 %\n",
      "epoch 71, minibatch 61/61, validation error 9.166667 %\n",
      "epoch 72, minibatch 61/61, validation error 9.354167 %\n",
      "epoch 73, minibatch 61/61, validation error 9.312500 %\n",
      "epoch 74, minibatch 61/61, validation error 9.520833 %\n",
      "epoch 75, minibatch 61/61, validation error 9.687500 %\n",
      "epoch 76, minibatch 61/61, validation error 9.104167 %\n",
      "epoch 77, minibatch 61/61, validation error 9.791667 %\n",
      "epoch 78, minibatch 61/61, validation error 9.604167 %\n",
      "epoch 79, minibatch 61/61, validation error 10.354167 %\n",
      "epoch 80, minibatch 61/61, validation error 21.083333 %\n",
      "epoch 81, minibatch 61/61, validation error 9.208333 %\n",
      "Optimization complete with best validation score of 8.583333 %.\n",
      "The code run for 82 epochs, with 0.382813 epochs/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code for file __file__ ran for 214.2s\n"
     ]
    }
   ],
   "source": [
    "while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in range(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "\n",
    "                    #test_losses = [test_model(i)\n",
    "                    #               for i in range(n_test_batches)]\n",
    "                    #test_score = numpy.mean(test_losses)\n",
    "                    #print(\n",
    "                    #    (\n",
    "                    #        '     epoch %i, minibatch %i/%i, test error of'\n",
    "                    #        ' best model %f %%'\n",
    "                    #    ) %\n",
    "                    #    (\n",
    "                    #        epoch,\n",
    "                    #        minibatch_index + 1,\n",
    "                    #        n_train_batches,\n",
    "                    #        test_score * 100.\n",
    "                    #    )\n",
    "                    #)\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'wb') as f:\n",
    "                        pickle.dump(classifier, f)\n",
    "                    print(\"Better then previous best function!\")\n",
    "                    \n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "print(\n",
    "    (\n",
    "        'Optimization complete with best validation score of %f %%.'\n",
    "    )\n",
    "    % (best_validation_loss * 100.)\n",
    ")\n",
    "print('The code run for %d epochs, with %f epochs/sec' % (\n",
    "    epoch, 1. * epoch / (end_time - start_time)))\n",
    "print(('The code for file ' +\n",
    "       os.path.split('__file__')[1] +\n",
    "       ' ran for %.1fs' % ((end_time - start_time))), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "So, we made a Logistic regression model, and updated its parameter based on min-batch Stochastic gradient descent.\n",
    "After 74 epochs, we could get the best test performance of roughly 93%. The model training reached the stopping criteria in about 2 minutes.\n",
    "\n",
    "We can now use this model for predicting the values on unseen datasets.Below is a function which can be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(shared_xx):\n",
    "    \"\"\"\n",
    "    An example of how to load a trained model and use it\n",
    "    to predict labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the saved model\n",
    "    #classifier = pickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # We can test it on some examples from test test\n",
    "    #dataset='mnist.pkl.gz'\n",
    "    #datasets = dl.load_data(dataset)\n",
    "    #test_set_x, test_set_y = datasets[2]\n",
    "    #test_set_x = test_set_x.get_value()\n",
    "\n",
    "    #predicted_values = predict_model(test_set_x[:100])\n",
    "    #xx = theano.shared(data)\n",
    "    predicted_values = predict_model(shared_xx.get_value())\n",
    "    return list(predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the Model on Kaggle competition\n",
    "\n",
    "Now, we will try to see the model accuracy on the dataset in kaggle. It contains 28000 images in the test set. As we used mode in training, a lot of the data would be exact points in our training data. This would lead to a buffed up result, but it would also act as a relative metric while comparing this model to the others we will be making, specifically multi-layer preceptron and Convoluted neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading the data\n",
    "csv_file = csv.reader(open(\"test.csv\"))\n",
    "header = csv_file.next()\n",
    "data = []\n",
    "for row in csv_file:\n",
    "    data.append(row)\n",
    "datax = numpy.array(data, dtype = \"float32\")\n",
    "test_x = theano.shared(numpy.asarray(data,\n",
    "                        dtype=theano.config.floatX),\n",
    "                        borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the predict() function\n",
    "test_labels = predict(test_x)\n",
    "\n",
    "# We need a serial ID array as well\n",
    "image_id = numpy.arange(1,len(test_labels)+1)\n",
    "\n",
    "#Now to write it to a csv file\n",
    "f = open(\"submission2.csv\", \"w\")\n",
    "f.write(\"{},{}\\n\".format(\"ImageId\", \"Label\"))\n",
    "for x in zip(image_id, test_labels):\n",
    "    f.write(\"{},{}\\n\".format(x[0], x[1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "We find the result to be 86% accurate. This is great for a model that gets trained in just two minutes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
