{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MNIST digits using Logistic Regression\n",
    "\n",
    "This notebook will show how Theano can be used to implement the logistic regression. As the plan, we have,\n",
    "\n",
    "1) A brief Intro to logistic Regression\n",
    "\n",
    "2) Loading data models\n",
    "\n",
    "3) Making the Logistic Regression Files\n",
    "\n",
    "4) Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "So, What is Logistic regression?\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "Mathematically, the probability that an input vector $x$ is a member of a class $i$, a value of a stochastic variable $Y$, can be written as:\n",
    "\\begin{align}\n",
    "P(Y = i \\mid x, W,b) &= softmax_i (Wx+b)\\\\\n",
    "&= \\frac{e^{W_i x + b_i}}{\\sum_{j}{{e}^{W_j x + b_j }}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "\\begin{equation}\n",
    "y_{pred} = argmax_i P(Y = i \\mid x,W,b)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We thus maximize the log-likelihood of our classifier given all the labels in a training set.\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) =\n",
    "    \\sum_{i=0}^{|\\mathcal{D}|} \\log P(Y=y^{(i)} | x^{(i)}, \\theta)\n",
    "\\end{equation}\n",
    "The likelihood of the correct class is not the same as the number of right predictions, but from the point of view of a randomly initialized classifier they are pretty similar. \n",
    "\n",
    "Since we usually speak in terms of minimizing a loss function, learning will thus attempt to minimize the negative log-likelihood (NLL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "What is ordinary gradient descent? it is a simple algorithm in which we repeatedly make small steps downward on an error surface defined by a loss function of some parameters. For the purpose of ordinary gradient descent we consider that the training data is rolled into the loss function.\n",
    "\n",
    "Stochastic gradient descent (SGD) works according to the same principles as ordinary gradient descent, but proceeds more quickly by estimating the gradient from just a few examples at a time instead of the entire training set. In its purest form, we estimate the gradient from just a single example at a time.\n",
    "\n",
    "The variant that is generally recommended for deep learning is a further twist on stochastic gradient descent using so-called “minibatches”. Minibatch SGD (MSGD) works identically to SGD, except that we use more than one training example to make each estimate of the gradient. This technique reduces variance in the estimate of the gradient, and often makes better use of the hierarchical memory organization in modern computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "while training our model from data, we are trying to prepare it to do well on new examples, not the ones it has already seen. The training loop above for MSGD does not take this into account, and may overfit the training examples. A way to combat overfitting is through regularization.\n",
    "\n",
    "#### L2 regularization\n",
    "L2 regularization involve adding an extra term to the loss function, which penalizes certain parameter configurations.\n",
    "\n",
    "If our loss function is\n",
    "\\begin{equation}\n",
    "NLL(\\theta, \\mathcal{D}) = - \\sum_{i=0}^{|\\mathcal{D}|} \\log P(Y=y^{(i)} | x^{(i)}, \\theta)\n",
    "\\end{equation}\n",
    "then the regularized loss will be:\n",
    "\\begin{equation}\n",
    "E(\\theta, \\mathcal{D}) =  NLL(\\theta, \\mathcal{D}) + \\lambda||\\theta||_2^2\n",
    "\\end{equation}\n",
    "where, $||\\theta||$ is the $L_2$ norm of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "You can either download the data from [Kaggle](https://www.kaggle.com/c/digit-recognizer/data), or from [The MNIST Database website](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "However, to understand the use of pickled data, we will load it from the existing picked file, [mnist.pkl.gz](mnist.pkl.gz)\n",
    "\n",
    "## Pickle \n",
    "\n",
    "Pickle is used for serializing and de-serializing a Python object structure. Any object in python can be pickled so that it can be saved on disk. For more information, check [this](https://pythontips.com/2013/08/02/what-is-pickle-in-python/) out.\n",
    "\n",
    "#### Now lets get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for loading the dataset\n",
    "# We define a function that takes the path to a file, and loads it.\n",
    "# Or downloads the file if not availible from University of Motreal's website.\n",
    "from code import dataset_loader as dl\n",
    "# train_set, valid_set, test_set format: tuple(input, target)\n",
    "# input is a numpy.ndarray of 2 dimensions (a matrix)\n",
    "# where each row corresponds to an example. target is a\n",
    "# numpy.ndarray of 1 dimension (vector) that has the same length as\n",
    "# the number of rows in the input. It should give the target\n",
    "# to the example with the same index in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the useful libraries and packages\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first define the Logistic regression class.\n",
    "It will have some basic definations,\n",
    "\n",
    "1) <code>\\_\\_init\\_\\_</code> : for the initialization. According the the number of inputs and outputs.\n",
    "\n",
    "2) <code> negative_log_likelihood </code> : For calculating the mean loss with respect to a given set of points of outcome variables(Minibatch).\n",
    "\n",
    "3)<code> error </code> : This gives the proportion of incorrectly labelled points.\n",
    "\n",
    "Kindly check the file, [LogisticRegression.py](code/LogisticRegression.py) to see how it has been coded. We will simply import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from code import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by allocating symbolic variables for the training inputs x and their corresponding classes $y$. Note that $x$ and $y$ are defined outside the scope of the <b> LogisticRegression object </b> . \n",
    "\n",
    "Since the class requires the input to build its graph, it is passed as a parameter of the __init__ function. This is useful in case you want to connect instances of such classes to form a deep network. The output of one layer can be passed as the input of the layer above.\n",
    "\n",
    "Finally, we define a (symbolic) cost variable to minimize, using the instance method <b>classifier.negative_log_likelihood</b> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the cost we minimize during training is the negative log likelihood of\n",
    "# the model in symbolic format\n",
    "cost = classifier.negative_log_likelihood(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{neg,no_inplace}.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "g_b = T.grad(cost=cost, wrt=classifier.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dot.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
