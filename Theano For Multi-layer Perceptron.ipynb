{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano and Lasagne for Multi-layer Perceptron\n",
    "\n",
    "The next architecture we are going to present using Theano is the single-hidden-layer Multi-Layer Perceptron (MLP). An MLP can be viewed as a logistic regression classifier where the input is first transformed using a learnt non-linear transformation $\\Phi$. This transformation projects the input data into a space where it becomes linearly separable. This intermediate layer is referred to as a hidden layer. A single hidden layer is sufficient to make MLPs a universal approximator. However we will see later on that there are substantial benefits to using many such hidden layers, i.e. the very premise of deep learning. \n",
    "\n",
    "## The Model\n",
    "\n",
    "Formally, a one-hidden-layer MLP is a function $f: R^D \\rightarrow R^L,$ where $D$ is the size of input vector $x$ and $L$ is the size of the output vector $f(x)$, such that, in matrix notation:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = G( b^{(2)} + W^{(2)}( s( b^{(1)} + W^{(1)} x))),\n",
    "\\end{equation}\n",
    "\n",
    "with bias vectors $b^{(1)}, b^{(2)}$; weight matrices $W^{(1)}, W^{(2)}$ and activation functions $G$ and $s$.\n",
    "\n",
    "The vector $h(x) = \\Phi(x) = s(b^{(1)} + W^{(1)} x)$ constitutes the hidden layer. $W^{(1)} \\in R^{D \\times D_h}$ is the weight matrix connecting the input vector to the hidden layer. Each column $W^{(1)}_{\\cdot i}$ represents the weights from the input units to the i-th hidden unit. Typical choices for $s$ include $tanh$, with $tanh(a)=(e^a-e^{-a})/(e^a+e^{-a})$, or the logistic sigmoid function, with $sigmoid(a)=1/(1+e^{-a})$. We will be using $ReLU = max(0,x)$. \n",
    "\n",
    "The output vector is then obtained as: $o(x) = G(b^{(2)} + W^{(2)} h(x))$. As before, class-membership probabilities can be obtained by choosing $G$ as the softmax function (in the case of multi-class classification).\n",
    "\n",
    "## Lasagne\n",
    "Lasagne is a lightweight library to build and train neural networks in Theano. we will be using this library for making our code more comprehensible.With Lasagne, we will be able to use better descent algorithms like Nestrove momentum, AdaGrad etc.\n",
    "\n",
    "We assume that you have already installed Theano. Now for installing Lasagne, open your anaconda prompt and type\n",
    "\n",
    "<code> conda install lasagne </code>\n",
    "\n",
    "Press 'y' to confirm installation and you are ready to go. \n",
    "\n",
    "You can clone the git repository of Lasagne by opening your GIT Bash, going to the location where you want it to be installed and typing\n",
    "\n",
    "<code> git clone https://github.com/Lasagne/Lasagne.git </code>\n",
    "\n",
    "Now, we have installed it in the same folder as the one in which this notebook exists.\n",
    "\n",
    "### Now lets get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "D:\\Anaconda2\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# Loading required directories\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A simple function to load the dataset.\n",
    "#from code import lasagne_dataset_loader as dl\n",
    "#X_train, y_train, X_val, y_val, X_test, y_test = dl.load_dataset()\n",
    "csv_file = csv.reader(open(\"train.csv\"))\n",
    "header = csv_file.next()\n",
    "data = []\n",
    "for row in csv_file:\n",
    "    data.append(row)\n",
    "data_full = np.array(data, dtype = \"float32\")\n",
    "datax = data_full[:,1:]\n",
    "datay = data_full[:,0:1]\n",
    "\n",
    "y_train = datay[0:37000,:]\n",
    "y_train.shape =(int(np.shape(y_train)[0]))\n",
    "y_train = y_train.astype(\"uint8\")\n",
    "y_val = datay[37000:,:]\n",
    "y_val.shape =(int(np.shape(y_val)[0]))\n",
    "y_val = y_val.astype('uint8')\n",
    "\n",
    "X_train = datax[0:37000,:]\n",
    "#X_train.shape = (37000,1,1,784)\n",
    "X_train.dtype = (\"float32\")\n",
    "X_val =datax[37000:,:]\n",
    "#X_val.shape = (5000,1,1,784)\n",
    "X_val.dtype = (\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "# Notice that this function returns only mini-batches of size `batchsize`.\n",
    "# If the size of the data is not a multiple of `batchsize`, it will not\n",
    "# return the last (remaining) mini-batch.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "The MLP we will be making will be an MLP of two hidden layers of 80 units each, followed by a softmax output layer of 10 units. It applies 20% dropout to the input data and 50% dropout to the hidden layers. It is similar, but not fully equivalent to the smallest MLP in [Hinton2012](http://lasagne.readthedocs.io/en/latest/user/tutorial.html#hinton2012) (that paper uses different nonlinearities, weight initialization and training). We can easily change this by changing the values in the function.\n",
    "\n",
    "## Glorot's scheme\n",
    "We use the Glorot's scheme for initialization. More can be read [here](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This creates an MLP of two hidden layers of 80 units each, followed by\n",
    "# a softmax output layer of 10 units. It applies 20% dropout to the input\n",
    "# data and 50% dropout to the hidden layers.\n",
    "\n",
    "#The four numbers in the shape tuple represent, in order: (batchsize, channels, rows, columns)\n",
    "# Input layer, specifying the expected input shape of the network\n",
    "# (unspecified batchsize, 1 channel, 1 rows and 28 columns) and\n",
    "# linking it to the given Theano variable `input_var`, if any:\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, 784))\n",
    "\n",
    "# Apply 20% dropout to the input data:\n",
    "l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "\n",
    "# Add a fully-connected layer of 80 units, using the linear rectifier, and\n",
    "# initializing weights with Glorot's scheme (which is the default anyway):\n",
    "l_hid1 = lasagne.layers.DenseLayer(\n",
    "    l_in_drop, num_units=80,\n",
    "    nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    W=lasagne.init.GlorotUniform())\n",
    "\n",
    "    # We'll now add dropout of 50%:\n",
    "l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)\n",
    "\n",
    "    # Another 80-unit layer:\n",
    "l_hid2 = lasagne.layers.DenseLayer(\n",
    "        l_hid1_drop, num_units=80,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # 50% dropout again:\n",
    "l_hid2_drop = lasagne.layers.DropoutLayer(l_hid2, p=0.5)\n",
    "    # Finally, we'll add the fully-connected output layer, of 10 softmax units:\n",
    "l_out = lasagne.layers.DenseLayer(\n",
    "        l_hid2_drop, num_units=10,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.matrix('inputs')\n",
    "target_var = T.ivector('targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(l_out)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(l_out,deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "test_classification = T.argmax(test_prediction,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([l_in.input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([l_in.input_var, target_var], [test_loss, test_acc])\n",
    "# for true test set\n",
    "predict_fn = theano.function([l_in.input_var],[test_prediction,test_classification])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1 took 1281.694s\n",
      "  training loss:\t\t358.849433\n",
      "  validation loss:\t\t71.046983\n",
      "  validation accuracy:\t\t14.04 %\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 10000, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Kaggle Test Set\n",
    "\n",
    "Now we will load the kaggle test csv file and find out the accuracy of the model in those data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading the data\n",
    "import csv\n",
    "csv_file = csv.reader(open(\"test.csv\"))\n",
    "header = csv_file.next()\n",
    "data = []\n",
    "for row in csv_file:\n",
    "    data.append(row)\n",
    "datax = np.array(data, dtype = \"float32\")\n",
    "#datax.shape = (int(np.shape(datax)[0]),1,1,784)\n",
    "datax.dtype = (\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8L"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted ,classes= predict_fn(datax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_labels = classes.tolist()\n",
    "image_id = np.arange(1,len(test_labels)+1)\n",
    "\n",
    "#Now to write it to a csv file\n",
    "f = open(\"submission2.csv\", \"w\")\n",
    "f.write(\"{},{}\\n\".format(\"ImageId\", \"Label\"))\n",
    "for x in zip(image_id, test_labels):\n",
    "    f.write(\"{},{}\\n\".format(x[0], x[1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now there is some problem with the prediction.\n",
    "\n",
    "The output contains the same values for all input rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
